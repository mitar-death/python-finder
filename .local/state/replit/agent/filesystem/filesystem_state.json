{"file_contents":{"Readme.md":{"content":"# Email Finder Application\n\n## Overview\nThis is a Python-based email finder application that searches for companies using various providers (Yelp, Google) and then finds email addresses for those companies using email discovery services (Hunter, Snov). The application is designed to help with lead generation and business intelligence.\n\n## Recent Changes\n- **September 20, 2025**: Fresh GitHub import setup in Replit environment\n  - Installed Python 3.11 and all required dependencies via pip\n  - Fixed pyproject.toml README reference to match actual filename\n  - Created config directory with template configuration files\n  - Set up \"Email Finder CLI\" workflow for console-based execution\n  - Validated application structure and core functionality\n  - Tested configuration validation and CLI help system\n  - Application ready for use with valid API keys\n\n## Project Architecture\n\n### Main Components\n- **main.py**: Entry point and orchestration logic\n- **config_loader.py**: Handles loading configuration from text files\n- **providers/**: Search providers that find companies (Yelp, Google)\n- **finders/**: Email discovery services (Hunter, Snov)\n- **config/**: Configuration files for API keys, queries, and settings\n- **output/**: Generated results files\n\n### Configuration Files\n- `config/providers.txt`: API keys for search providers (format: provider=API_KEY)\n- `config/email_finders.txt`: API keys for email finders (format: finder=API_KEY)\n- `config/queries.txt`: Search queries, one per line\n- `config/proxies.txt`: Proxy configurations (currently disabled)\n\n### Workflow\n1. Load configuration from text files\n2. Run search providers to find companies based on queries\n3. Extract domains from company URLs\n4. Use email finders to discover emails for those domains\n5. Save results to output files\n\n### Current Status\n- ✅ Python 3.11 environment configured in Replit\n- ✅ All dependencies installed via pip from pyproject.toml\n- ✅ Console workflow \"Email Finder CLI\" configured and tested\n- ✅ Configuration files created with placeholder API keys\n- ✅ Application structure validated and ready for use\n- ✅ CLI interface tested and working (--help, --validate-config)\n- ✅ Pipeline execution tested successfully\n- ⚠️ Requires valid API keys for Yelp and Hunter services to function\n\n### Known Limitations\n- SOCKS proxy support disabled (requires additional dependencies)\n- Google provider requires valid API key and custom search engine ID\n- Some type annotation warnings remain but don't affect functionality\n\n## Setup Instructions\n1. **Add API Keys**: Edit the configuration files to add your real API keys:\n   - `config/providers.txt`: Replace `test_yelp_key_placeholder` with your Yelp API key\n   - `config/email_finders.txt`: Replace `test_hunter_key_placeholder` with your Hunter API key\n\n2. **Customize Search Queries**: Edit `config/queries.txt` to add your own search terms\n\n3. **Run the Application**: The \"Email Finder CLI\" workflow will automatically execute the pipeline\n\n## Usage\nThe application runs via the \"Email Finder CLI\" workflow. Results are saved in the `output/` directory:\n- `companies.txt`: Found company information\n- `domains.txt`: Extracted domain names  \n- `emails.txt`: Discovered email addresses\n\n## User Preferences\n- Console-based application (no frontend required)\n- Extensible provider and finder architecture\n- Configuration via text files for easy management\n\n\n## Issues and To-Dos\n- if the the providers.txt, email_finders.txt, email.txt. and there are 2 entries with the same key eg if i have 2 hunter= with differnet apikeys, that should be seen as 2 diffrent classes, so eg in that case, and use it as 2 diffrent provider or finders.","size_bytes":3697},"pyproject.toml":{"content":"[project]\nname = \"leadgen\"\nversion = \"0.1.0\"\ndescription = \"Extensible CLI app for business lead generation\"\npackage-mode = false\nauthors = [\n    {name = \"Anonymous User\",email = \"golobal@gmail.com\"}\n]\nreadme = \"Readme.md\"\nrequires-python = \">=3.10\"\ndependencies = [\n    \"requests[socks] (>=2.32.5,<3.0.0)\",\n    \"typer[all]>=0.9.0\",\n    \"rich>=13.0.0\",\n    \"loguru>=0.7.0\",\n    \"pydantic>=2.0.0\",\n    \"pyyaml>=6.0.0\",\n    \"tenacity>=8.0.0\",\n    \"openpyxl (>=3.1.5,<4.0.0)\"\n]\n\n\n[tool.poetry.scripts]\nleadgen= \"leadgen.cli.main:main\"\n\n[build-system]\nrequires = [\"poetry-core>=2.0.0,<3.0.0\"]\nbuild-backend = \"poetry.core.masonry.api\"\n","size_bytes":631},"run_leadgen.py":{"content":"\"\"\"Run the full lead generation pipeline with the new CLI.\"\"\"\nimport sys\nimport os\n\n# Add src to path for importing\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))\n\ntry:\n    from leadgen.cli.main import main\n    print(\"Running full lead generation pipeline...\")\n    \n    # Set up arguments for full run\n    sys.argv = ['run_leadgen.py', '--verbose', '--delay', '5']\n    main()\n    \nexcept Exception as e:\n    print(f\"Error running new CLI: {e}\")\n    import traceback\n    traceback.print_exc()","size_bytes":511},"leadgen/__init__.py":{"content":"","size_bytes":0},"leadgen/main.py":{"content":"\"\"\"Main entry point for the lead generation CLI.\"\"\"\n\n# For backward compatibility, import and run the CLI\nfrom .cli.main import main\n\nif __name__ == \"__main__\":\n    main()","size_bytes":171},"leadgen/orchestrator.py":{"content":"\"\"\"Main orchestrator for the lead generation process.\"\"\"\nimport sys\nimport time\nimport random\nfrom typing import Dict, List, Set, Optional\nfrom leadgen.utils.proxy import ProxyManager\nfrom .config.loader import ConfigLoader, ConfigurationError\nfrom .config.models import AppConfig\nfrom .models.company import Company\nfrom .models.email_result import EmailResult\nfrom .providers.base import BaseProvider, ProviderError\nfrom .providers.yelp import YelpProvider\nfrom .finders.base import BaseFinder\nfrom .finders.hunter import HunterFinder\nfrom .utils.logging import logger\nfrom .utils.domain import DomainResolver\nfrom leadgen.domain_finders.base import BaseDomainFinder, BaseDomainFinder\nfrom leadgen.domain_finders.hunter import HunterDomainFinder\n\n\nclass LeadOrchestrator:\n    \"\"\"Orchestrates the lead generation process.\"\"\"\n\n    def __init__(self, config: AppConfig, state_store=None, args=None):\n        self.config = config\n        self.state_store = state_store\n        self.providers: Dict[str, BaseProvider] = {}\n        self.finders: Dict[str, BaseFinder] = {}\n        self.domain_finders: Dict[str, BaseDomainFinder] = {}\n        # Results storage\n        self.companies: List[Company] = []\n        self.domains: Set[str] = set()\n        self.email_results: List[EmailResult] = []\n        self.proxy_index = 0\n        self.args = args\n\n        self._initialize_providers()\n        self._initialize_domain_finders()\n        self._initialize_finders()\n\n    def _initialize_providers(self):\n        \"\"\"Initialize search providers.\"\"\"\n        provider_map = {\n            \"yelp\": YelpProvider,\n            # \"google\": GoogleProvider,  # Will add when dependencies are available\n        }\n\n        for name, api_keys in self.config.providers.items():\n            if name.lower() in provider_map:\n                for i, api_key in enumerate(api_keys):\n                    try:\n                        provider_config = {}\n                        if name.lower() == \"yelp\":\n                            provider_config = {\n                                \"location\": self.config.location,\n                                \"limit\": self.config.yelp_limit\n                            }\n\n                        provider = provider_map[name.lower()](api_key,\n                                                              provider_config)\n                        # Use unique keys for multiple instances of same provider type\n                        provider_key = f\"{name}_{i+1}\" if len(\n                            api_keys) > 1 else name\n                        self.providers[provider_key] = provider\n                        logger.info(f\"Initialized {provider_key} provider\")\n\n                    except Exception as e:\n                        logger.error(\n                            f\"Failed to initialize {name} provider instance {i+1}: {e}\"\n                        )\n            else:\n                logger.warning(f\"Unknown provider: {name}\")\n\n    def _initialize_domain_finders(self):\n        \"\"\"Initialize email finders.\"\"\"\n        domain_finder_map = {\n            #  \"apollo\": ApolloDomainFinder,\n            \"hunter\": HunterDomainFinder,\n        }\n\n        domain_finders = ConfigLoader()._load_providers('domain_finders.txt')\n        for name, api_keys in domain_finders.items():\n            if name.lower() in domain_finder_map:\n                for i, api_key in enumerate(api_keys):\n                    try:\n                        finder = domain_finder_map[name.lower()](api_key)\n                        # Use unique keys for multiple instances of same finder type\n                        finder_key = f\"{name}_{i+1}\" if len(\n                            api_keys) > 1 else name\n                        self.domain_finders[finder_key] = finder\n                        logger.info(f\"Initialized {finder_key} domain finder\")\n\n                    except Exception as e:\n                        logger.error(\n                            f\"Failed to initialize {name} domain finder instance {i+1}: {e}\"\n                        )\n            else:\n                logger.warning(f\"Unknown domain finder: {name}\")\n\n    def _initialize_finders(self):\n        \"\"\"Initialize email finders.\"\"\"\n        finder_map = {\n            \"hunter\": HunterFinder,\n\n            # \"snov\": SnovFinder,  # Will add when needed\n        }\n\n        for name, api_keys in self.config.email_finders.items():\n            if name.lower() in finder_map:\n                for i, api_key in enumerate(api_keys):\n                    try:\n                        finder = finder_map[name.lower()](api_key)\n                        # Use unique keys for multiple instances of same finder type\n                        finder_key = f\"{name}_{i+1}\" if len(\n                            api_keys) > 1 else name\n                        self.finders[finder_key] = finder\n                        logger.info(f\"Initialized {finder_key} email finder\")\n\n                    except Exception as e:\n                        logger.error(\n                            f\"Failed to initialize {name} email finder instance {i+1}: {e}\"\n                        )\n            else:\n                logger.warning(f\"Unknown email finder: {name}\")\n\n    def run_provider_search(self):\n        \"\"\"Run the provider search phase.\"\"\"\n        if not self.providers:\n            logger.warning(\"No providers configured, skipping search phase\")\n            return\n\n        if not self.config.queries:\n            logger.warning(\"No queries configured, skipping search phase\")\n            return\n\n        logger.info(\"Starting provider search phase\")\n        logger.info(\n            f\"Searching with {len(self.providers)} providers for {len(self.config.queries)} queries\"\n        )\n        all_companies = []\n        for provider_name, provider in self.providers.items():\n            logger.info(f\"Running {provider_name} provider\")\n\n            for query in self.config.queries:\n                try:\n                    proxy = ProxyManager()._get_proxy()\n                    proxy_info = f\"proxy {proxy}\" if proxy else \"no proxy\"\n                    logger.info(\n                        f\"[{provider_name.upper()}] Searching '{query}'\")\n\n                    companies = provider.search(query, proxy)\n\n                    logger.success(\n                        f\"Found {len(companies)} companies for '{query}'\")\n\n                    all_companies.extend(companies)\n                    # Rate limiting between requests\n                    if self.config.delays.request_delay > 0:\n                        time.sleep(self.config.delays.request_delay)\n\n                except ProviderError as e:\n                    logger.error(\n                        f\"[{provider_name.upper()}] Provider error for '{query}': {e}\"\n                    )\n                    continue\n                except Exception as e:\n                    logger.error(\n                        f\"[{provider_name.upper()}] Unexpected error for '{query}': {e}\"\n                    )\n                    continue\n\n            # Delay between providers\n            if self.config.delays.provider_delay > 0:\n                logger.info(\n                    f\"Waiting {self.config.delays.provider_delay}s before next provider\"\n                )\n                time.sleep(self.config.delays.provider_delay)\n\n        # Filter companies using StateStore for proper deduplication\n        if self.state_store:\n            new_companies = []\n            skipped_count = 0\n            for company in all_companies:\n                if not self.state_store.is_seen_company(company):\n                    new_companies.append(company)\n                    self.state_store.add_seen_company(company)\n                else:\n                    skipped_count += 1\n\n            logger.info(\n                f\"Found {len(new_companies)} new companies (skipped {skipped_count} already processed)\"\n            )\n            self.companies = new_companies\n        else:\n            # Fallback to old logic if no state store\n            old_companies = ConfigLoader()._load_companies(f\"companies.txt\")\n            filtered_companies = [\n                company for company in all_companies\n                if company.name not in old_companies\n            ]\n            logger.info(f\"Found {len(filtered_companies)} new companies\")\n            self.companies = filtered_companies\n        logger.success(\n            f\"Search phase complete: {len(self.companies)} companies\")\n\n        # Save state after company search phase\n        if self.state_store:\n            self.state_store.save_state()\n            logger.debug(\"💾 State saved after company search phase\")\n\n    def run_email_discovery(self):\n        \"\"\"Run the email discovery phase.\"\"\"\n        if not self.finders:\n            logger.warning(\n                \"No email finders configured, skipping email discovery\")\n            return\n\n        if not self.domains:\n            logger.warning(\"No domains found, skipping email discovery\")\n            return\n\n        # Wait before starting email discovery\n        if self.config.delays.finder_delay > 0:\n            logger.info(\n                f\"Waiting {self.config.delays.finder_delay}s before starting email discovery\"\n            )\n            time.sleep(self.config.delays.finder_delay)\n\n        logger.info(\"Starting email discovery phase\")\n        logger.info(\n            f\"Searching emails for {len(self.domains)} domains using {len(self.finders)} finders\"\n        )\n\n        # Filter domains that haven't been processed yet\n        domains_to_process = []\n        if self.state_store:\n            for domain in self.domains:\n                if not self.state_store.is_seen_domain(domain):\n                    domains_to_process.append(domain)\n                else:\n                    logger.info(\n                        f\"Domain {domain} already processed for emails, skipping\"\n                    )\n        else:\n            domains_to_process = list(self.domains)\n\n        logger.info(\n            f\"Processing emails for {len(domains_to_process)} new domains\")\n\n        for finder_name, finder in self.finders.items():\n            logger.info(f\"Running {finder_name} email finder\")\n\n            for domain in domains_to_process:\n                try:\n                    proxy = ProxyManager()._get_proxy()\n                    proxy_info = f\"proxy {proxy}\" if proxy else \"no proxy\"\n                    logger.info(\n                        f\"[{finder_name.upper()}] Finding emails for '{domain}'\"\n                    )\n\n                    result = finder.find_email(domain, proxy)\n                    self.email_results.append(result)\n\n                    if result.success and result.emails:\n                        # Mark emails as seen in StateStore\n                        if self.state_store:\n                            for email_obj in result.emails:\n                                if hasattr(email_obj,\n                                           'email') and email_obj.email:\n                                    self.state_store.add_seen_email(\n                                        email_obj.email)\n                        logger.success(\n                            f\"Found {len(result.emails)} emails for '{domain}'\"\n                        )\n                    elif result.success:\n                        logger.info(f\"No emails found for '{domain}'\")\n                    else:\n                        logger.warning(\n                            f\"Failed to find emails for '{domain}': {result.error}\"\n                        )\n\n                    # Rate limiting between requests\n                    if self.config.delays.request_delay > 0:\n                        time.sleep(self.config.delays.request_delay)\n\n                except Exception as e:\n                    logger.error(\n                        f\"[{finder_name.upper()}] Unexpected error for '{domain}': {e}\"\n                    )\n                    # Create failed result\n                    failed_result = EmailResult(domain=domain,\n                                                emails=[],\n                                                finder=finder_name,\n                                                success=False,\n                                                error=str(e))\n                    self.email_results.append(failed_result)\n\n                    continue\n\n        successful_results = [\n            r for r in self.email_results if r.success and r.emails\n        ]\n        logger.success(\n            f\"Email discovery complete: {len(successful_results)} domains with emails found\"\n        )\n\n    def run_domain_discovery(self):\n        \"\"\"\n        Extract business domain from Yelp business page.\n        \"\"\"\n        logger.info(\"Starting domain discovery phase\")\n        resolver = DomainResolver()\n\n        for company in self.companies:\n            company_name = getattr(company, \"name\", \"UNKNOWN\")\n\n            # This flag ensures we stop searching once a valid result is found for a single company\n            found_result = False\n\n            if self.domain_finders:\n                finders_iter = self.domain_finders.items() if isinstance(\n                    self.domain_finders, dict) else self.domain_finders\n\n                for name, domain_finder in finders_iter:\n                    if found_result:\n                        break  # Move to the next company if we already found a result\n\n                    try:\n                        proxy = ProxyManager()._get_proxy()\n                        proxy_info = f\"proxy {proxy}\" if proxy else \"no proxy\"\n                        logger.info(\n                            f\"[{name.upper()}] Finding emails for '{company_name}'\"\n                        )\n                        res = domain_finder.find(\n                            company,\n                            proxy=proxy)  #hwhere safe_request is called\n\n                        if res and isinstance(res, str):\n                            # Domain found\n                            domain = resolver._clean_and_extract_domain(res)\n                            if domain and resolver._is_valid_business_domain(\n                                    domain):\n                                # Check if domain already processed (StateStore integration)\n                                if self.state_store and self.state_store.is_seen_domain(\n                                        domain):\n                                    logger.info(\n                                        f\"Domain {domain} already processed, skipping\"\n                                    )\n                                else:\n                                    company.domain = domain\n                                    self.domains.add(domain)\n                                    if self.state_store:\n                                        self.state_store.add_seen_domain(\n                                            domain)\n                                    logger.info(\n                                        f\"Found domain {domain} for {company_name}\"\n                                    )\n                                found_result = True\n\n                        else:\n                            # For hunter domain finder that returns emails instead of domains\n                            domain = res[\"data\"][\"domain\"]\n                            contacts = domain_finder._parse_email_data(res)\n\n                            # Check if domain already processed\n                            if self.state_store and self.state_store.is_seen_domain(\n                                    domain):\n                                logger.info(\n                                    f\"Domain {domain} already processed, skipping\"\n                                )\n                            else:\n                                # Emails found\n                                result = EmailResult(domain=domain,\n                                                     emails=contacts,\n                                                     finder=name,\n                                                     success=True)\n\n                                self.domains.add(domain)\n                                if self.state_store:\n                                    self.state_store.add_seen_domain(domain)\n                                    # Also mark emails as seen\n                                    for contact in contacts:\n                                        if hasattr(contact,\n                                                   'email') and contact.email:\n                                            self.state_store.add_seen_email(\n                                                contact.email)\n                            # Always mark as processed, even if we skipped it\n                            if self.state_store and domain:\n                                self.state_store.add_seen_domain(domain)\n                                self.email_results.append(\n                                    result\n                                )  # Correctly store the EmailResult object\n                                logger.info(\n                                    f\"Found {len(res['data']['emails'])} emails for {company_name}\"\n                                )\n                            found_result = True\n\n                    except Exception as e:\n                        logger.error(type(e))\n                        logger.error(\n                            f\"Error for {company_name} with finder {[name.upper()]}: {e}\"\n                        )\n\n                        logger.info(\"selecting a new domain finder\")\n                        # You can log the error but continue to the next finder\n                        continue\n\n        logger.success(\n            f\"Domain discovery complete: {len(self.domains)} valid domains found and {len(self.email_results)} email results collected\"\n        )\n\n    def run_full_pipeline(self):\n        \"\"\"Run the complete lead generation pipeline.\"\"\"\n        logger.info(\"Starting lead generation pipeline\")\n\n        try:\n            if self.config.run_email_finder_alone:\n                self.run_email_discovery()\n            else:\n                # run only if args has fresh\n                # if self.args.fresh:\n                self.run_provider_search()\n                self.run_domain_discovery()\n\n                #only run this if the email_results is empty after processing domain discovery\n                if not self.email_results:\n                    self.run_email_discovery()\n                logger.success(\n                    \"Lead generation pipeline completed successfully\")\n\n        except KeyboardInterrupt:\n            logger.warning(\"Pipeline interrupted by user\")\n        except Exception as e:\n            logger.error(f\"Pipeline failed with unexpected error: {e}\")\n            raise\n","size_bytes":18886},"leadgen/cli/__init__.py":{"content":"","size_bytes":0},"leadgen/cli/main.py":{"content":"\"\"\"Simple CLI interface for lead generation.\"\"\"\nimport sys\nimport argparse\nfrom pathlib import Path\nfrom typing import List, Tuple\nfrom leadgen.utils.proxy import ProxyManager\nfrom ..config.loader import ConfigLoader, ConfigurationError\nfrom ..orchestrator import LeadOrchestrator\nfrom ..utils.logging import logger\nfrom ..io.storage import OutputManager\nfrom ..utils.state import StateStore\n\n\ndef create_parser() -> argparse.ArgumentParser:\n    \"\"\"Create command line argument parser.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"Lead generation CLI tool\",\n        formatter_class=argparse.RawDescriptionHelpFormatter\n    )\n    \n    parser.add_argument(\n        \"--config-dir\",\n        default=\"config\",\n        help=\"Configuration directory (default: config)\"\n    )\n    \n    parser.add_argument(\n        \"--location\",\n        default=\"United States\",\n        help=\"Location of this business, including address, city, state, zip code and country.\"\n    )\n    parser.add_argument(\n        \"--hunter-department\",\n        default=\"United States\",\n        help=\"Get only email addresses for people working in the selected department(s\"\n    )\n    \n    parser.add_argument(\n        \"--output-dir\", \n        default=\"output\",\n        help=\"Output directory (default: output)\"\n    )\n    \n    parser.add_argument(\n        \"--delay\",\n        type=float,\n        help=\"Override finder delay in seconds\"\n    )\n    \n    parser.add_argument(\n        \"--validate-config\",\n        action=\"store_true\",\n        help=\"Validate configuration and exit\"\n    )\n    \n    parser.add_argument(\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Enable verbose logging\"\n    )\n    \n    # Mode selection - mutually exclusive\n    mode_group = parser.add_mutually_exclusive_group()\n    mode_group.add_argument(\n        \"--resume\",\n        action=\"store_true\",\n        default=True,\n        help=\"Resume from previous run (default - skips already processed data)\"\n    )\n    mode_group.add_argument(\n        \"--fresh\",\n        action=\"store_true\",\n        help=\"Start fresh run (clears existing output data after confirmation)\"\n    )\n    \n    return parser\n\n\ndef validate_config_command(config_dir: str) -> bool:\n    \"\"\"Validate configuration and show status.\"\"\"\n    try:\n        loader = ConfigLoader(config_dir)\n        config = loader.load_config()\n        \n        logger.success(\"Configuration is valid\")\n        logger.info(f\"Providers: {list(config.providers.keys())}\")\n        logger.info(f\"Email finders: {list(config.email_finders.keys())}\")\n        logger.info(f\"Queries: {len(config.queries)} configured\")\n        logger.info(f\"Proxies: {len(config.proxies)} configured\")\n        \n        return True\n        \n    except ConfigurationError as e:\n        logger.error(f\"Configuration error: {e}\")\n        return False\n\n\ndef check_output_safety(output_dir: str, output_config) -> Tuple[bool, List[str]]:\n    \"\"\"Check if output files exist and return file info.\"\"\"\n    output_path = Path(output_dir)\n    existing_files = []\n    \n    if not output_path.exists():\n        return True, []\n    \n    # Check for existing output files\n    file_extensions = [\"txt\", \"csv\", \"jsonl\", \"json\", \"xlsx\"]\n    file_bases = [output_config.companies_file, output_config.domains_file, output_config.emails_file]\n    \n    for base in file_bases:\n        for ext in file_extensions:\n            file_path = output_path / f\"{base}.{ext}\"\n            if file_path.exists() and file_path.stat().st_size > 0:\n                file_size = file_path.stat().st_size\n                size_str = f\"{file_size:,} bytes\"\n                if file_size > 1024:\n                    size_str = f\"{file_size/1024:.1f} KB\"\n                if file_size > 1024*1024:\n                    size_str = f\"{file_size/(1024*1024):.1f} MB\"\n                    \n                existing_files.append(f\"{file_path.name} ({size_str})\")\n                \n    return len(existing_files) == 0, existing_files\n\n\ndef confirm_fresh_run(existing_files: List[str]) -> bool:\n    \"\"\"Ask user confirmation for fresh run that will clear data.\"\"\"\n    logger.warning(\"⚠️  EXISTING OUTPUT FILES DETECTED:\")\n    for file_info in existing_files:\n        logger.warning(f\"   📄 {file_info}\")\n        \n    logger.warning(\"\\n🔥 Running with --fresh will PERMANENTLY DELETE this data!\")\n    logger.info(\"\\nOptions:\")\n    logger.info(\"  • Continue with --fresh (DELETE existing data)\")\n    logger.info(\"  • Cancel and run with --resume (PRESERVE existing data)\")\n    \n    while True:\n        try:\n            response = input(\"\\nContinue with fresh run? [y/N]: \").strip().lower()\n            if response in ['y', 'yes']:\n                return True\n            elif response in ['', 'n', 'no']:\n                return False\n            else:\n                print(\"Please enter 'y' for yes or 'n' for no.\")\n        except (KeyboardInterrupt, EOFError):\n            print(\"\\nOperation cancelled.\")\n            return False\n\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = create_parser()\n    args = parser.parse_args()\n    \n    # Set logging level\n    if args.verbose:\n        logger.level = \"DEBUG\"\n    \n    # Validate config if requested\n    if args.validate_config:\n        success = validate_config_command(args.config_dir)\n        sys.exit(0 if success else 1)\n        \n   \n     \n    \n    try:\n        # Load configuration\n        logger.info(\"Loading configuration...\")\n        loader = ConfigLoader(args.config_dir)\n        config = loader.load_config()\n        \n        # Override delay if specified\n        if args.delay is not None:\n            config.delays.finder_delay = args.delay\n            config.delays.domain_delay = args.delay\n            logger.info(f\"Using custom finder and domain delay: {args.delay}s\")\n        \n        # Update output directory\n        config.output.directory = args.output_dir\n        \n        if location := args.location:\n            config.location = location\n            logger.info(f\"Using custom location: {location}\")\n            \n        if hunter_department := args.hunter_department:\n            config.hunter_department = hunter_department\n            logger.info(f\"Using custom hunter_department: {hunter_department}\")\n        \n        # Check for existing output files and handle mode selection\n        is_safe, existing_files = check_output_safety(args.output_dir, config.output)\n        \n        if not is_safe and existing_files:\n            if args.fresh:\n                # Fresh mode - ask for confirmation before deleting\n                if not confirm_fresh_run(existing_files):\n                    logger.info(\"Operation cancelled by user\")\n                    sys.exit(0)\n                logger.info(\"🔥 Starting fresh run - existing data will be cleared\")\n            else:\n                # Resume mode (default) - show info about existing data\n                logger.info(\"📁 Existing output files detected - running in RESUME mode:\")\n                for file_info in existing_files:\n                    logger.info(f\"   📄 {file_info}\")\n                logger.info(\"   ℹ️  Already processed data will be skipped\")\n                logger.info(\"   💡 Use --fresh to start over or --resume to continue\")\n        \n        # Initialize StateStore\n        state_store = StateStore(args.output_dir, config.output)\n        \n        if args.fresh and existing_files:\n            # Clear existing state and files\n            state_store.clear_state()\n            # Remove existing output files\n            output_path = Path(args.output_dir)\n            file_extensions = [\"txt\", \"csv\", \"jsonl\", \"json\", \"xlsx\"]\n            file_bases = [config.output.companies_file, config.output.domains_file, config.output.emails_file]\n            \n            # Delete specific output files\n            for base in file_bases:\n                for ext in file_extensions:\n                    file_path = output_path / f\"{base}.{ext}\"\n                    if file_path.exists():\n                        file_path.unlink()\n                        logger.info(f\"🗑️  Deleted {file_path.name}\")\n            \n            # Also clear .state directory\n            state_dir = output_path / \".state\"\n            if state_dir.exists():\n                import shutil\n                shutil.rmtree(state_dir)\n                logger.info(\"🗑️  Cleared state cache\")\n            \n            logger.info(\"🗑️  Fresh run: all existing output data cleared\")\n        else:\n            # Load existing state for resume mode\n            logger.info(\"📖 Loading existing state for resume...\")\n            state_store.load_from_output()\n            stats = state_store.get_stats()\n            if any(stats.values()):\n                logger.info(f\"   📊 Found: {stats['companies']} companies, {stats['domains']} domains, {stats['emails']} emails\")\n        \n        # Create orchestrator and run with state store\n        orchestrator = LeadOrchestrator(config, state_store, args)\n        orchestrator.run_full_pipeline()\n        \n        # Save results\n        logger.info(\"Saving results...\")\n        output_manager = OutputManager(config.output)\n        output_manager.save_results(\n            companies=orchestrator.companies,\n            email_results=orchestrator.email_results,\n            filtered_domains=orchestrator.domains  # Pass the filtered domains\n        )\n        \n        # Final state save\n        if state_store:\n            state_store.save_state()\n            logger.info(\"💾 State saved successfully\")\n        \n        logger.success(\"Lead generation completed successfully\")\n        \n    except ConfigurationError as e:\n        logger.error(f\"Configuration error: {e}\")\n        logger.error(\"Run with --validate-config to check your configuration\")\n        sys.exit(1)\n        \n    except KeyboardInterrupt:\n        logger.warning(\"Interrupted by user\")\n        sys.exit(1)\n        \n    except Exception as e:\n        logger.error(f\"Unexpected error: {e}\")\n        if args.verbose:\n            import traceback\n            traceback.print_exc()\n        sys.exit(1)\n\n\nif __name__ == \"__main__\":\n    main()","size_bytes":10135},"leadgen/config/__init__.py":{"content":"","size_bytes":0},"leadgen/config/loader.py":{"content":"\"\"\"Configuration loader with validation and error handling.\"\"\"\nimport os\nfrom pathlib import Path\nfrom typing import Dict, List\nfrom .models import AppConfig\n\nfrom leadgen.utils.logging import logger\nclass ConfigurationError(Exception):\n    \"\"\"Raised when configuration is invalid or missing.\"\"\"\n    pass\n\n\nclass ConfigLoader:\n    \"\"\"Loads and validates configuration from files.\"\"\"\n    \n    def __init__(self, config_dir: str = \"config\", output_dir =\"output\"):\n        self.config_dir = Path(config_dir)\n        self.output_dir = Path(output_dir)\n    \n    def load_config(self) -> AppConfig:\n        \"\"\"Load complete application configuration.\"\"\"\n        config = AppConfig.from_env()\n        \n        # Load from files, with file values taking precedence over defaults\n        # but environment variables taking precedence over file values\n        file_providers = self._load_providers(\"providers.txt\")\n        file_finders = self._load_providers(\"email_finders.txt\")\n        \n        # Merge with environment overrides\n        for name, keys in file_providers.items():\n            if name not in config.providers:  # Don't override env vars\n                config.providers[name] = keys\n            else:\n                # Append file keys to existing env keys\n                config.providers[name].extend(keys)\n                \n        for name, keys in file_finders.items():\n            if name not in config.email_finders:  # Don't override env vars\n                config.email_finders[name] = keys\n            else:\n                # Append file keys to existing env keys\n                config.email_finders[name].extend(keys)\n        \n        config.proxies = self._load_proxies(\"proxies.txt\")\n        config.queries = self._load_queries(\"queries.txt\")\n        \n        self._validate_config(config)\n        return config\n    \n    def _load_providers(self, filename: str) -> Dict[str, List[str]]:\n        \"\"\"Load provider configuration from file, supporting multiple API keys per provider.\"\"\"\n        file_path = self.config_dir / filename\n        \n        if not file_path.exists():\n            return {}\n            \n        providers = {}\n        try:\n            lines = self._read_lines(file_path)\n            for line_num, line in enumerate(lines, 1):\n                if \"=\" not in line:\n                    raise ConfigurationError(\n                        f\"{filename}:{line_num}: Invalid format. \"\n                        f\"Expected 'provider=API_KEY', got '{line}'\"\n                    )\n                \n                name, key = line.split(\"=\", 1)\n                name, key = name.strip(), key.strip()\n                \n                if not name or not key:\n                    raise ConfigurationError(\n                        f\"{filename}:{line_num}: Empty provider name or API key\"\n                    )\n                \n                # Support multiple API keys for the same provider type\n                if name not in providers:\n                    providers[name] = []\n                providers[name].append(key)\n                \n        except Exception as e:\n            if isinstance(e, ConfigurationError):\n                raise\n            raise ConfigurationError(f\"Error reading {filename}: {e}\")\n            \n        return providers\n    \n    def _load_proxies(self, filename: str) -> List[str]:\n        \"\"\"Load proxy configuration from file.\"\"\"\n        file_path = self.config_dir / filename\n        \n        if not file_path.exists():\n            return []\n            \n        try:\n            return self._read_lines(file_path)\n        except Exception as e:\n            raise ConfigurationError(f\"Error reading {filename}: {e}\")\n    \n    def _load_searchable_domains(self, filename):\n        \"\"\"Load searchable domains from file.\"\"\"\n        file_path = self.config_dir / filename\n\n        if not file_path.exists():\n            return []\n\n        try:\n            return self._read_lines(file_path)\n        except Exception as e:\n            raise ConfigurationError(f\"Error reading searchable_domains.txt: {e}\")\n    def _load_queries(self, filename: str) -> List[str]:\n        \"\"\"Load search queries from file.\"\"\"\n        file_path = self.config_dir / filename\n        \n        if not file_path.exists():\n            raise ConfigurationError(\n                f\"Required file {filename} not found. \"\n                f\"Please create it with one search query per line.\"\n            )\n            \n        try:\n            queries = self._read_lines(file_path)\n            if not queries:\n                raise ConfigurationError(\n                    f\"{filename} is empty. Please add at least one search query.\"\n                )\n            return queries\n        except Exception as e:\n            if isinstance(e, ConfigurationError):\n                raise\n            raise ConfigurationError(f\"Error reading {filename}: {e}\")\n        \n    def _load_companies(self, filename: str) -> List[str]:\n        \"\"\"Load companies from file.\"\"\"\n        file_path = self.output_dir / filename\n\n        if not file_path.exists():\n            raise ConfigurationError(\n                f\"Required file {filename} not found. \"\n                f\"Please create it with one company name per line.\"\n            )\n\n        try:\n            companies = self._read_lines(file_path)\n            if not companies:\n                raise ConfigurationError(\n                    f\"{filename} is empty. Please add at least one company name.\"\n                )\n            return companies\n        except Exception as e:\n            if isinstance(e, ConfigurationError):\n                raise\n            raise ConfigurationError(f\"Error reading {filename}: {e}\")\n    \n    def _read_lines(self, file_path: Path) -> List[str]:\n        \"\"\"Read non-empty, non-comment lines from file.\"\"\"\n        lines = []\n        \n        with file_path.open(\"r\", encoding=\"utf-8\") as f:\n            for line_num, line in enumerate(f, 1):\n                line = line.strip()\n                \n                # Skip empty lines and comments\n                if not line or line.startswith(\"#\"):\n                    continue\n                    \n                lines.append(line)\n        \n        return lines\n    \n    def _validate_config(self, config: AppConfig) -> None:\n        \"\"\"Validate the loaded configuration.\"\"\"\n        # Check that we have at least one provider or email finder\n        if not config.providers and not config.email_finders:\n            raise ConfigurationError(\n                \"No providers or email finders configured. \"\n                \"Please add API keys to providers.txt or email_finders.txt\"\n            )\n        \n        # Validate queries\n        if not config.queries:\n            raise ConfigurationError(\n                \"No search queries configured. \"\n                \"Please add queries to queries.txt\"\n            )\n            \n        # Check for required Google CX if Google provider is enabled\n        if \"google\" in config.providers and not config.google_cx:\n            if not os.getenv(\"GOOGLE_CX\"):\n                print(\n                    \"Warning: Google provider enabled but no Custom Search Engine ID (CX) configured. \"\n                    \"Set GOOGLE_CX environment variable or update google_cx in config.\"\n                )","size_bytes":7285},"leadgen/config/models.py":{"content":"\"\"\"Configuration models.\"\"\"\nfrom dataclasses import dataclass, field\nfrom typing import Dict, List, Optional\nimport os\n\n\n@dataclass\nclass DelayConfig:\n    \"\"\"Configuration for delays between operations.\"\"\"\n    provider_delay: float = 0.0  # seconds between provider requests\n    finder_delay: float = 1.0  # seconds before starting email finding\n    domain_delay: float = 5.0  # seconds between domain requests\n    request_delay: float = 1.0  # seconds between individual requests\n\n\n@dataclass\nclass ProxyConfig:\n    \"\"\"Configuration for proxy settings.\"\"\"\n    enabled: bool = True\n    rotation: str = \"round_robin\"  # round_robin, random\n    timeout: float = 10.0\n\n\n@dataclass\nclass OutputConfig:\n    \"\"\"Configuration for output settings.\"\"\"\n    format: str = \"xlsx\"  # jsonl, csv, txt,xlxs\n    directory: str = \"output\"\n    companies_file: str = \"companies\"\n    domains_file: str = \"domains\"\n    emails_file: str = \"emails\"\n\n\n@dataclass\nclass AppConfig:\n    \"\"\"Main application configuration.\"\"\"\n    providers: Dict[str, List[str]] = field(default_factory=dict)\n    email_finders: Dict[str, List[str]] = field(default_factory=dict)\n    proxies: List[str] = field(default_factory=list)\n    queries: List[str] = field(default_factory=list)\n    run_email_finder_alone: bool = False\n\n    # Sub-configurations\n    delays: DelayConfig = field(default_factory=DelayConfig)\n    proxy_config: ProxyConfig = field(default_factory=ProxyConfig)\n    output: OutputConfig = field(default_factory=OutputConfig)\n\n    # Provider-specific settings\n    location: str = \"United States\"  # which location should the provider serach\n    yelp_limit: int = 50\n    google_cx: str = \"\"\n    google_limit: int = 5\n\n    # Email finder specific settings\n    email_finder_limit: int = 2\n\n    # Get only email addresses for people working in the selected department(s).\n    # The possible values are executive, it, finance, management, sales, legal,\n    # support, hr, marketing, communication, education, design, health or\n    # operations. Several departments can be selected (comma-delimited).\n    hunter_department: str = \"\"\n\n    @classmethod\n    def from_env(cls) -> \"AppConfig\":\n        \"\"\"Create config with environment variable overrides.\"\"\"\n        config = cls()\n\n        # Override with environment variables if present\n        if hunter_key := os.getenv(\"HUNTER_API_KEY\"):\n            config.email_finders[\"hunter\"] = [hunter_key]\n        if snov_key := os.getenv(\"SNOV_API_KEY\"):\n            config.email_finders[\"snov\"] = [snov_key]\n        if yelp_key := os.getenv(\"YELP_API_KEY\"):\n            config.providers[\"yelp\"] = [yelp_key]\n        if google_key := os.getenv(\"GOOGLE_API_KEY\"):\n            config.providers[\"google\"] = [google_key]\n        if google_cx := os.getenv(\"GOOGLE_CX\"):\n            config.google_cx = google_cx\n\n        return config\n","size_bytes":2837},"leadgen/domain_finders/apollo.py":{"content":"\"\"\"Hunter.io email finder implementation.\"\"\"\nimport requests\nfrom typing import Optional, Dict\nfrom leadgen.utils.logging import logger\nfrom leadgen.models.company import Company\nfrom .base import BaseDomainFinder, DomainFinderError\n\nclass ApolloDomainFinder(BaseDomainFinder):\n    \"\"\"apollo.io domain discovery service.\"\"\"\n    \n    BASE_URL = \"https://api.apollo.io/api/v1/mixed_companies/search\"\n    \n    def __init__(self, api_key):\n        super().__init__(api_key=api_key)\n        if not self.api_key:\n            raise DomainFinderError(\"No Hunter.io API key provided\")\n        self.headers ={\n                    'Content-Type': 'application/json',\n                    'X-Api-Key': self.api_key,\n                    'accept': 'application/json'\n            }\n    \n    @property\n    def name(self) -> str:\n        return \"hunter\"\n    \n    def find(self, company: Company, proxy: Optional[Dict[str, str]] = None) -> str:\n        \"\"\"Find emails for a domain using Hunter.io API.\"\"\"\n        if not company.domain:\n            logger.info(f\"Company {company.name} has no domain, skipping\")\n            return None\n        params = {\"q_organization_name\": company.domain}\n        \n        try:\n            logger.info(f\"Searching apllo.io for {company.name}\")\n            response = requests.post(\n                    self.BASE_URL,\n                    params=params,\n                    proxies=proxy,\n                    timeout=10,\n                    headers=self.headers\n                )\n\n            \n            response.raise_for_status()\n            logger.info(f\"Apollo.io returns {response} for email {company.domain}\")\n            data = response.json()\n            \n            # Extract emails from Hunter.io response\n            email_data = data.get(\"data\", {})\n            emails = [\n                email.get(\"value\", \"\")\n                for email in email_data.get(\"emails\", [])\n                if email.get(\"value\")\n            ]\n            \n            return emails\n            \n        except requests.HTTPError as e:\n            error_msg = f\"Apollo.io API error: {e}\"\n            return e\n        except requests.RequestException as e:\n            error_msg = f\"Network error contacting apollo.io: {e}\"\n            return e\n        except Exception as e:\n            error_msg = f\"Unexpected error in Apollo search: {e}\"\n            return e","size_bytes":2369},"leadgen/domain_finders/base.py":{"content":"from abc import ABC, abstractmethod\nfrom typing import Optional, Dict, Any, List\nfrom leadgen.models.company import Company\nfrom leadgen.models.email_result import Contact\n\nclass BaseDomainFinder(ABC):\n    \"\"\"Abstract class for all domain finders\"\"\"\n\n    \n    def __init__(self, api_key: str, config: Optional[Dict[str, Any]] = None):\n        self.api_key = api_key\n        self.config = config or {}\n        \n        \n    @abstractmethod\n    def find(self, company: Company, proxy: Optional[Dict[str, str]] = None) -> str|List[Contact]:\n        \"\"\"\n        Perform a domain search and return the domain.\n        \n        Args:\n            company: company object\n            proxy: Optional proxy configuration dict (e.g., {\"http\": \"...\", \"https\": \"...\"})\n            \n        Returns:\n            a valid Company domain\n            \n        Raises:\n            DomainFinderError If the domain search fails\n        \"\"\"\n        pass\n    \n    @property\n    \n    \n    \n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Return the domain name.\"\"\"\n        pass\n\n\nclass DomainFinderError(Exception):\n    \"\"\"Raised when a domain search encounters an error.\"\"\"\n    pass","size_bytes":1170},"leadgen/domain_finders/hunter.py":{"content":"\"\"\"Hunter.io email finder implementation.\"\"\"\nimport requests\nfrom typing import List, Dict, Optional, Any\nfrom leadgen.utils.logging import logger\nfrom leadgen.models.company import Company\nfrom leadgen.models.email_result import Contact\nfrom leadgen.utils.proxy import ProxyError, ProxyManager\nfrom leadgen.config.loader import ConfigLoader\nfrom leadgen.config.models import AppConfig\nfrom .base import BaseDomainFinder, DomainFinderError\n\n\nclass HunterDomainFinder(BaseDomainFinder):\n    \"\"\"Hunter.io domain discovery service.\"\"\"\n\n    BASE_URL = \"https://api.hunter.io/v2/domain-search\"\n\n    @property\n    def name(self) -> str:\n        return \"hunter\"\n\n    def find(self,\n             company: Company,\n             proxy: Optional[Dict[str, str]] = None) -> str | Dict[str, Any]:\n        \"\"\"Find emails for a company using Hunter.io API.\n        Returns \n            can be str or contacts\n        \"\"\"\n        config: AppConfig = ConfigLoader().load_config()\n        params = {\n            \"company\": company.name,\n            \"api_key\": self.api_key,\n            \"department\": config.hunter_department or \"executive\",\n            \"limit\": config.email_finder_limit or 10\n        }\n\n        try:\n            response = ProxyManager().safe_request(\"get\",\n                                                   self.BASE_URL,\n                                                   params=params,\n                                                   proxies=proxy,\n                                                   timeout=10)\n\n            if response.status_code == 429:\n                logger.debug(f\"Hunter.io API rate limit exceeded\")\n                raise DomainFinderError(\"Hunter.io API rate limit exceeded\")\n            if response is None:\n                raise DomainFinderError(\"No response from Hunter.io\")\n\n            data = response.json()\n\n            # Debug logging for problematic responses\n            logger.debug(f\"Hunter domain finder response for {company.name}: {data}\")\n            \n            # Extract emails from Hunter.io response\n            return data\n\n        except ProxyError as e:\n            logger.debug(\"Hunter Proxy Execption called\")\n            raise DomainFinderError(f\"Hunter.io proxy error: {e}\")\n        except requests.HTTPError as e:\n            logger.debug(\"Hunter HTTPError called\")\n            raise DomainFinderError(f\"Hunter.io API error: {e}\")\n        except requests.RequestException as e:\n            logger.debug(\"Hunter Request Execption called\")\n            raise DomainFinderError(f\"Network error contacting Hunter.io: {e}\")\n        except Exception as e:\n            logger.debug(\"Hunter Exception called\")\n            raise Exception(f\"Unexpected error in Hunter search: {e}\")\n\n    def _parse_email_data(self, data: dict) -> List[Contact]:\n        \"\"\"\n        Parse Hunter.io / email finder JSON and return Contact objects\n        \"\"\"\n        contacts = []\n\n        email_data = data.get(\"data\", {})\n        company_name = email_data.get(\"organization\") or \"Unknown\"\n\n        emails_list = email_data.get(\"emails\", [])\n\n        for email_entry in emails_list:\n            first_name = email_entry.get(\"first_name\") or \"\"\n            last_name = email_entry.get(\"last_name\") or \"\"\n            contact = Contact(name=f\"{first_name} {last_name}\".strip(),\n                              email=email_entry.get(\"value\", \"\"),\n                              company_name=company_name,\n                              position=email_entry.get(\"position\") or email_entry.get(\"seniority\") or \"\")\n            contacts.append(contact)\n\n        # logger.info(f\"Parsed {len(contacts)} contacts from response data\")\n        return contacts\n","size_bytes":3677},"leadgen/finders/__init__.py":{"content":"","size_bytes":0},"leadgen/finders/base.py":{"content":"\"\"\"Base finder interface for email discovery services.\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Dict\nfrom leadgen.models.email_result import Contact\nfrom ..models.email_result import EmailResult\n\n\nclass BaseFinder(ABC):\n    \"\"\"Abstract base class for all email finders.\"\"\"\n    \n    def __init__(self, api_key: str):\n        self.api_key = api_key\n    \n    @abstractmethod\n    def find_email(self, domain: str, proxy: Optional[Dict[str, str]] = None) -> EmailResult:\n        \"\"\"\n        Find emails for a domain and return structured result.\n        \n        Args:\n            domain: Domain name to search for emails\n            proxy: Optional proxy configuration dict\n            \n        Returns:\n            EmailResult object with emails found and metadata\n        \"\"\"\n        pass\n    \n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Return the finder name.\"\"\"\n        pass\n    \n    @abstractmethod\n    def _parse_email_data(self, data: dict) -> List[Contact]:\n        \"\"\"\n        Parse  email finder JSON and return Contact objects\n        \"\"\"\n        pass\n\n\nclass FinderError(Exception):\n    \"\"\"Raised when a finder encounters an error.\"\"\"\n    pass","size_bytes":1213},"leadgen/finders/hunter.py":{"content":"\"\"\"Hunter.io email finder implementation.\"\"\"\nimport requests\nfrom typing import Optional, Dict, List, Any\nfrom leadgen.utils.logging import logger\nfrom leadgen.utils.proxy import ProxyManager\nfrom leadgen.config.loader import ConfigLoader\nfrom leadgen.config.models import AppConfig\nfrom .base import BaseFinder, FinderError\nfrom ..models.email_result import EmailResult, Contact\n\n\nclass HunterFinder(BaseFinder):\n    \"\"\"Hunter.io email discovery service.\"\"\"\n    \n    BASE_URL = \"https://api.hunter.io/v2/domain-search\"\n    \n    @property\n    def name(self) -> str:\n        return \"hunter\"\n    \n    def find_email(self, domain: str, proxy: Optional[Dict[str, str]] = None) -> EmailResult:\n        \"\"\"Find emails for a domain using Hunter.io API.\"\"\"\n        config: AppConfig = ConfigLoader().load_config()\n\n        params = {\"domain\": domain, \"api_key\": self.api_key,\"department\": config.hunter_department or \"executive\"}\n        \n        try:\n            response = ProxyManager().safe_request(\"get\",\n                self.BASE_URL,\n                params=params,\n                proxies=proxy,\n                timeout=10\n            )\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            # Extract emails from Hunter.io response\n            logger.debug(f\"Hunter.io returns {data} for email {domain}\")\n            \n            contacts = self._parse_email_data(data)\n            \n            return EmailResult(\n                domain=domain,\n                emails=contacts,\n                finder=self.name,\n                success=True\n            )\n            \n        except requests.HTTPError as e:\n            error_msg = f\"Hunter.io API error: {e}\"\n            return EmailResult(\n                domain=domain,\n                emails=[],\n                finder=self.name,\n                success=False,\n                error=error_msg\n            )\n        except requests.RequestException as e:\n            error_msg = f\"Network error contacting Hunter.io: {e}\"\n            return EmailResult(\n                domain=domain,\n                emails=[],\n                finder=self.name,\n                success=False,\n                error=error_msg\n            )\n        except Exception as e:\n            error_msg = f\"Unexpected error in Hunter search: {e}\"\n            return EmailResult(\n                domain=domain,\n                emails=[],\n                finder=self.name,\n                success=False,\n                error=error_msg\n            )\n        \n        \n    def _parse_email_data(self, data: dict) -> List[Contact]:\n        \"\"\"\n        Parse Hunter.io / email finder JSON and return Contact objects\n        \"\"\"\n        contacts = []\n    \n        email_data = data.get(\"data\", {})\n        company_name = email_data.get(\"organization\") or \"\"\n\n        emails_list = email_data.get(\"emails\", [])\n\n        for email_entry in emails_list:\n            first_name = email_entry.get(\"first_name\") or \"\"\n            last_name = email_entry.get(\"last_name\") or \"\"\n            contact = Contact(\n                name=f\"{first_name} {last_name}\".strip(),\n                email=email_entry.get(\"value\", \"\"),\n                company_name=company_name,\n                position=email_entry.get(\"position\", \"\")\n            )\n            contacts.append(contact)\n\n        # logger.info(f\"Parsed {len(contacts)} contacts from response data\")\n        return contacts","size_bytes":3443},"leadgen/io/__init__.py":{"content":"","size_bytes":0},"leadgen/io/storage.py":{"content":"import json\nimport csv\nfrom pathlib import Path\nfrom typing import List, Optional, Set\nfrom openpyxl import Workbook\nfrom ..models.company import Company\nfrom ..models.email_result import EmailResult\nfrom ..config.models import OutputConfig\nfrom ..utils.logging import logger\n\n\nclass OutputManager:\n    \"\"\"Manages saving results (companies, domains, emails) to various formats.\"\"\"\n\n    def __init__(self, config: OutputConfig):\n        self.config = config\n        self.output_dir = Path(config.directory)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n\n    # -------------------- PUBLIC --------------------\n\n    def save_results(\n        self,\n        companies: List[Company],\n        email_results: List[EmailResult],\n        filtered_domains: Optional[Set[str]] = None,\n    ):\n        \"\"\"Save all results to configured output format.\"\"\"\n        logger.info(f\"Saving results to {self.output_dir}\")\n\n        # Companies\n        if companies:\n            self._save_companies(companies)\n\n        # Domains\n        if filtered_domains is not None:\n            logger.info(f\"Filtering {len(filtered_domains)} domains\")\n            self._save_domains_filtered(filtered_domains)\n        else:\n            self._save_domains(companies)\n\n        # Emails\n        if email_results:\n            self._save_emails(email_results)\n\n        logger.success(f\"Results saved to {self.output_dir}\")\n\n    # -------------------- COMPANIES --------------------\n\n    def _save_companies(self, companies: List[Company]):\n        \"\"\"Save company data based on configured format.\"\"\"\n        if self.config.format.lower() in {\"jsonl\", \"json\"}:\n            self._save_jsonl([c.to_dict() for c in companies], self.config.companies_file)\n        elif self.config.format.lower() == \"csv\":\n            self._save_csv([c.to_dict() for c in companies], self.config.companies_file)\n        else:\n            self._save_txt([c.to_dict() for c in companies], self.config.companies_file)\n\n    # -------------------- DOMAINS --------------------\n\n    def _save_domains(self, companies: List[Company]):\n        \"\"\"Extract unique valid domains from companies.\"\"\"\n        from ..utils.domain import DomainResolver\n\n        resolver = DomainResolver()\n        domains = {c.domain for c in companies if c.domain and resolver._is_valid_business_domain(c.domain)}\n        self._save_domains_set(domains, self.config.domains_file)\n\n    def _save_domains_filtered(self, domains: Set[str]):\n        \"\"\"Save pre-filtered domains from orchestrator.\"\"\"\n        self._save_domains_set(domains, self.config.domains_file)\n\n    def _save_domains_set(self, domains: Set[str], filename: str):\n        \"\"\"Save a set of domains in TXT or CSV.\"\"\"\n        if not domains:\n            logger.info(\"No domains to save - writing empty file\")\n            with open(self.output_dir / f\"{filename}.txt\", \"w\", encoding=\"utf-8\") as f:\n                pass\n            return\n\n        # Remove None values and ensure strings\n        cleaned_domains = [str(d) for d in domains if d is not None and d]\n\n        if self.config.format.lower() == \"csv\":\n            file_path = self.output_dir / f\"{filename}.csv\"\n            with open(file_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n                writer = csv.writer(f)\n                writer.writerow([\"domain\"])\n                for d in sorted(cleaned_domains):\n                    writer.writerow([d])\n        else:  # TXT\n            file_path = self.output_dir / f\"{filename}.txt\"\n            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n                for d in sorted(cleaned_domains):\n                    f.write(d + \"\\n\")\n\n        logger.info(f\"Saved {len(cleaned_domains)} domains to {file_path}\")\n\n    # -------------------- EMAILS --------------------\n\n    def _save_emails(self, email_results: List[EmailResult]):\n        \"\"\"Save emails in configured format.\"\"\"\n        emails_list = []\n        for r in email_results:\n            if r.success and r.emails:\n                emails_cleaned = [ {k: v for k, v in e.to_dict().items() if k != \"domain\"} for e in r.emails ]\n                emails_list.extend(emails_cleaned)\n\n        if not emails_list:\n            logger.info(\"No email results to save\")\n            return\n\n        if self.config.format.lower() == \"csv\":\n            self._save_csv(emails_list, self.config.emails_file)\n        elif self.config.format.lower() in {\"jsonl\", \"json\"}:\n            self._save_jsonl(emails_list, self.config.emails_file)\n        elif self.config.format.lower() == \"xlsx\":\n            self._save_xlsx(emails_list, self.config.emails_file)\n        else:\n            self._save_txt(emails_list, self.config.emails_file)\n\n    # -------------------- FILE HELPERS --------------------\n\n    def _save_txt(self, data: List[dict], filename: str):\n        file_path = self.output_dir / f\"{filename}.txt\"\n        with open(file_path, \"a\", encoding=\"utf-8\") as f:\n            for entry in data:\n                f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n        logger.info(f\"Saved {len(data)} records to {file_path}\")\n\n    def _save_csv(self, data: List[dict], filename: str):\n        if not data:\n            return\n        file_path = self.output_dir / f\"{filename}.csv\"\n        fieldnames = sorted({k for d in data for k in d.keys()})\n        \n        # Check if file exists and has content to avoid duplicate headers\n        file_exists = file_path.exists() and file_path.stat().st_size > 0\n        \n        with open(file_path, \"a\", newline=\"\", encoding=\"utf-8\") as f:\n            writer = csv.DictWriter(f, fieldnames=fieldnames)\n            if not file_exists:\n                writer.writeheader()\n            writer.writerows(data)\n        logger.info(f\"Saved {len(data)} records to {file_path}\")\n\n    def _save_jsonl(self, data: List[dict], filename: str):\n        file_path = self.output_dir / f\"{filename}.jsonl\"\n        with open(file_path, \"a\", encoding=\"utf-8\") as f:\n            for entry in data:\n                json.dump(entry, f, ensure_ascii=False)\n                f.write(\"\\n\")\n        logger.info(f\"Saved {len(data)} records to {file_path}\")\n\n    def _save_xlsx(self, data: List[dict], filename: str):\n        \"\"\"Append data to XLSX file, creating a new file if it doesn't exist.\"\"\"\n        if not data:\n            return\n        file_path = self.output_dir / f\"{filename}.xlsx\"\n        from openpyxl import load_workbook, Workbook\n\n        if file_path.exists():\n            wb = load_workbook(file_path)\n            ws = wb.active\n        else:\n            wb = Workbook()\n            ws = wb.active\n            if ws is not None:\n                # Write header\n                fieldnames = sorted({k for d in data for k in d.keys()})\n                ws.append(fieldnames)\n\n        if ws is not None:\n            fieldnames = [cell.value for cell in ws[1]]  # Use existing header\n            for row in data:\n                ws.append([row.get(f, \"\") for f in fieldnames])\n\n        wb.save(file_path)\n        logger.info(f\"Appended {len(data)} records to {file_path}\")\n","size_bytes":7052},"leadgen/models/__init__.py":{"content":"","size_bytes":0},"leadgen/models/company.py":{"content":"\"\"\"Company data model.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import Optional\nfrom urllib.parse import urlparse\n\n\n@dataclass\nclass Company:\n    \"\"\"Represents a company found by a search provider.\"\"\"\n    id: str\n    name: str\n    url: Optional[str] = None\n    domain: Optional[str] = None\n    address: Optional[str] = None\n    phone: Optional[str] = None\n\n    def __post_init__(self):\n        \"\"\"Extract domain from URL if not provided and it's not a provider URL.\"\"\"\n        if self.url and self.domain is None:\n            try:\n                parsed = urlparse(self.url)\n                extracted_domain = parsed.netloc\n\n                # Only use extracted domain if it's not a provider domain\n                from ..utils.domain import DomainResolver\n                resolver = DomainResolver()\n                if resolver._is_valid_business_domain(extracted_domain):\n                    self.domain = extracted_domain\n                # If it's a provider domain, leave domain as None\n            except Exception:\n                self.domain = None\n\n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"id\": self.id,\n            \"name\": self.name,\n            \"url\": self.url,\n            \"domain\": self.domain,\n            \"address\": self.address,\n            \"phone\": self.phone,\n        }\n","size_bytes":1370},"leadgen/models/email_result.py":{"content":"\"\"\"Email result data model.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n\n\nfrom dataclasses import dataclass\nfrom typing import List, Optional\n\n@dataclass\nclass Contact:\n    \"represnts a persons contact\"\n    name: str   \n    company_name: str\n    email: str\n    position: str\n    \n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"name\": self.name,\n            \"company_name\": self.company_name,\n            \"email\": self.email,\n            \"position\": self.position\n        }\n    \n@dataclass\nclass EmailResult:\n    \"\"\"Represents emails found for a domain.\"\"\"\n    domain: str\n    emails: List[Contact]\n    finder: str\n    success: bool = True\n    error: Optional[str] = None\n    \n    def to_dict(self) -> dict:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        # This is the key change. We iterate over the list of Contact objects\n        # and call .to_dict() on each one.\n        serialized_emails = [contact.to_dict() for contact in self.emails]\n        \n        return {\n            \"domain\": self.domain,\n            \"emails\": serialized_emails,  # Use the new, serialized list\n            \"finder\": self.finder,\n            \"success\": self.success,\n            \"error\": self.error,\n        }","size_bytes":1303},"leadgen/providers/__init__.py":{"content":"","size_bytes":0},"leadgen/providers/base.py":{"content":"\"\"\"Base provider interface for search providers.\"\"\"\nfrom abc import ABC, abstractmethod\nfrom typing import List, Optional, Dict, Any\nfrom ..models.company import Company\n\n\nclass BaseProvider(ABC):\n    \"\"\"Abstract base class for all search providers.\"\"\"\n    \n    def __init__(self, api_key: str, config: Optional[Dict[str, Any]] = None):\n        self.api_key = api_key\n        self.config = config or {}\n    \n    @abstractmethod\n    def search(self, query: str, proxy: Optional[Dict[str, str]] = None) -> List[Company]:\n        \"\"\"\n        Perform a search and return a list of Company objects.\n        \n        Args:\n            query: Search query string\n            proxy: Optional proxy configuration dict (e.g., {\"http\": \"...\", \"https\": \"...\"})\n            \n        Returns:\n            List of Company objects found\n            \n        Raises:\n            ProviderError: If the search fails\n        \"\"\"\n        pass\n    \n    @property\n    @abstractmethod\n    def name(self) -> str:\n        \"\"\"Return the provider name.\"\"\"\n        pass\n\n\nclass ProviderError(Exception):\n    \"\"\"Raised when a provider encounters an error.\"\"\"\n    pass","size_bytes":1137},"leadgen/providers/yelp.py":{"content":"\"\"\"Yelp search provider implementation.\"\"\"\nimport requests\nfrom typing import List, Optional, Dict\nfrom leadgen.utils.proxy import ProxyManager\nfrom .base import BaseProvider, ProviderError\nfrom ..models.company import Company\nfrom ..utils.domain import DomainResolver\nfrom ..utils.logging import logger\n\nclass YelpProvider(BaseProvider):\n    \"\"\"Yelp business search provider.\"\"\"\n\n    BASE_URL = \"https://api.yelp.com/v3/businesses/search\"\n\n    def __init__(self, api_key: str, config: Optional[Dict] = None):\n        super().__init__(api_key, config)\n        self.headers = {\"Authorization\": f\"Bearer {self.api_key}\"}\n        self.domain_resolver = DomainResolver()\n        self.config = config or {}\n\n    @property\n    def name(self) -> str:\n        return \"yelp\"\n\n    def search(self,\n               query: str,\n               proxy: Optional[Dict[str, str]] = None) -> List[Company]:\n        \"\"\"Search for businesses using Yelp API.\"\"\"\n        params = {\n            \"term\": query,\n            \"location\": self.config.get(\"location\", \"United States\"),\n            \"limit\": self.config.get(\"limit\", 5),\n            \"is_claimed\": True,\n        }\n\n        try:\n            response = ProxyManager().safe_request(\"get\", self.BASE_URL,\n                                    headers=self.headers,\n                                    params=params,\n                                    proxies=proxy,\n                                    timeout=10)\n\n            if response.status_code != 200:\n                raise ProviderError(\n                    f\"Yelp API request failed with status code {response.status_code}: \"\n                    f\"{response.text}\")\n\n            data = response.json()\n            businesses = data.get(\"businesses\", [])\n\n            companies = []\n            for business in businesses:\n                id = business.get(\"id\", \"\")\n                name = business.get(\"name\", \"\")\n                yelp_url = business.get(\"url\", \"\")\n                address = \", \".join(\n                    business.get(\"location\", {}).get(\"display_address\", []))\n\n                \n                company = Company(\n                    id=id,\n                    name=name,\n                    url=yelp_url,  # Keep Yelp URL for reference\n                    address=address,\n                    phone=business.get(\"phone\", \"\"))\n                \n                \n                # # Try to resolve the actual business website domain\n                # business_domain = None\n                # if yelp_url and name:\n                #     business_domain = self.domain_resolver.extract_business_domain(company)\n\n                # # Only set domain if we found a valid business domain (not provider domain)\n                # valid_domain = None\n                # if business_domain and self.domain_resolver._is_valid_business_domain(\n                #         business_domain):\n                #     valid_domain = business_domain\n                #     logger.info(f\"Found valid domain: {valid_domain} with id {id}\")\n                    \n                # # set valid domain\n                # company.domain = valid_domain\n                companies.append(company)\n            return companies\n\n        except requests.RequestException as e:\n            raise ProviderError(f\"Network error while searching Yelp: {e}\")\n        except Exception as e:\n            raise ProviderError(f\"Unexpected error in Yelp search: {e}\")\n","size_bytes":3422},"leadgen/utils/__init__.py":{"content":"","size_bytes":0},"leadgen/utils/domain.py":{"content":"\"\"\"Domain utilities for business website resolution.\"\"\"\nimport re\nimport requests\nfrom urllib.parse import urlparse, urljoin\nfrom typing import Optional, Set\nfrom leadgen.config.loader import ConfigLoader\nfrom leadgen.domain_finders.base import BaseDomainFinder\nfrom leadgen.models.company import Company\nfrom leadgen.models.email_result import Contact\nfrom ..utils.logging import logger\nfrom typing import List, Dict, Any\nimport time\n\nclass DomainResolver:\n    \"\"\"Resolves business domains from provider data.\"\"\"\n    \n    # Provider domains to exclude from email discovery\n    PROVIDER_DOMAINS = {\n        \"yelp.com\", \"www.yelp.com\",\n        \"google.com\", \"www.google.com\", \"maps.google.com\",\n        \"facebook.com\", \"www.facebook.com\", \"m.facebook.com\",\n        \"instagram.com\", \"www.instagram.com\",\n        \"twitter.com\", \"www.twitter.com\", \"x.com\",\n        \"linkedin.com\", \"www.linkedin.com\",\n        \"foursquare.com\", \"www.foursquare.com\",\n        \"yellowpages.com\", \"www.yellowpages.com\"\n    }\n    \n    def __init__(self, timeout: int = 10):\n        self.timeout = timeout\n        self.session = requests.Session()\n        self.session.headers.update({\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n        })\n\n    def extract_business_domain(self, company: Company ) -> Optional[str]:\n        \"\"\"\n        Extract business domain from Yelp business page.\n        \n        Args:\n            company: the company object\n            \n        Returns:\n            Business domain if found, None otherwise\n        \"\"\"\n        company_name = getattr(company, \"name\", \"UNKNOWN\")\n        try:\n            logger.info(f\"Resolving domain for {company_name} using {len(self.domain_finders) if self.domain_finders else 0} finders\")\n\n            if self.domain_finders:\n                # Support either dict or list of tuples\n                if isinstance(self.domain_finders, dict):\n                    finders_iter = self.domain_finders.items()\n                else:\n                    finders_iter = self.domain_finders\n\n                for name, domain_finder in finders_iter:\n                    proxy = getattr(Proxy(), \"_get_proxy\", lambda: None)()\n                    proxy_info = f\"proxy {proxy}\" if proxy else \"no proxy\"\n                    logger.info(f\"[{name.upper()}] Searching for '{company_name}' with {proxy_info}\")\n\n                    res = domain_finder.find(company, proxy=proxy)\n                    if res and isinstance(res, str):\n                        domain = self._clean_and_extract_domain(res)\n                        if domain and self._is_valid_business_domain(domain):\n                            logger.debug(f\"Found business domain {domain} via {name} for {company_name}\")\n                            return domain\n                    elif res and isinstance(res, list):\n                        emails: List[Contact] = res\n                    \n\n            # Sleep between attempts for rate limiting\n            time.sleep(5)\n            logger.warning(f\"Could not find business website for {company_name}\")\n            return None\n\n        except Exception as e:\n            logger.error(f\"Error resolving domain for {company_name}: {e}\")\n            return e\n    \n\n    def _search_business_website(self, company_name: str, address: str) -> Optional[str]:\n        \"\"\"\n        Basic business website search using company name and address.\n        Note: This is a simplified implementation. In production, you'd use:\n        - Google Custom Search API\n        - Business data APIs (Clearbit, FullContact, etc.)\n        - Domain intelligence services\n        \n        return \n            a string of the business domain\n        \"\"\"\n        # For now, return None as we don't have search API setup\n        # This would be implemented with proper search APIs in production\n        logger.debug(f\"Business website search not implemented for {company_name}\")\n        return None\n    \n    def _clean_and_extract_domain(self, url: str) -> Optional[str]:\n        \"\"\"Clean URL and extract domain.\"\"\"\n        try:\n            # Handle relative URLs and clean up\n            if url.startswith('//'):\n                url = 'https:' + url\n            elif url.startswith('/'):\n                return None  # Skip relative URLs without base\n            elif not url.startswith(('http://', 'https://')):\n                url = 'https://' + url\n            \n            parsed = urlparse(url)\n            domain = parsed.netloc.lower()\n            \n            # Remove common prefixes\n            if domain.startswith('www.'):\n                domain = domain[4:]\n                \n            return domain if domain else None\n            \n        except Exception:\n            return None\n    \n    def _is_provider_domain(self, domain: str) -> bool:\n        \"\"\"Check if domain is a provider domain that should be excluded.\"\"\"\n        if not domain:\n            return True\n            \n        domain = domain.lower()\n        if domain.startswith('www.'):\n            domain = domain[4:]\n            \n        return domain in self.PROVIDER_DOMAINS or any(\n            provider in domain for provider in ['yelp', 'google', 'facebook', 'instagram']\n        )\n    \n    def _is_valid_business_domain(self, domain: str) -> bool:\n        \"\"\"Validate that domain is suitable for business email discovery.\"\"\"\n        if type(domain) != str:\n            return False\n        \n        if not domain or self._is_provider_domain(domain):\n            return False\n        logger.info(f\"Validating domain: {domain}\")\n            \n        # Basic domain validation\n        if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9-]*[a-zA-Z0-9]*\\.[a-zA-Z]{2,}$', domain):\n            return False\n            \n        # Exclude common non-business domains\n        excluded_patterns = [\n            'gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com',\n            'aol.com', 'icloud.com', 'live.com', 'msn.com'\n        ]\n        \n        return not any(pattern in domain.lower() for pattern in excluded_patterns)\n    \n    def filter_valid_domains(self, domains: Set[str]) -> Set[str]:\n        \"\"\"Filter set of domains to include only valid business domains.\"\"\"\n        return {domain for domain in domains if self._is_valid_business_domain(domain)}","size_bytes":6272},"leadgen/utils/logging.py":{"content":"\"\"\"Basic logging utilities until Rich/Loguru are available.\"\"\"\nimport sys\nfrom datetime import datetime\nfrom typing import Optional\n\n\nclass Logger:\n    \"\"\"Simple logger with levels and colors.\"\"\"\n    \n    # ANSI color codes\n    COLORS = {\n        \"DEBUG\": \"\\033[36m\",    # Cyan\n        \"INFO\": \"\\033[32m\",     # Green\n        \"WARNING\": \"\\033[33m\",  # Yellow\n        \"ERROR\": \"\\033[31m\",    # Red\n        \"RESET\": \"\\033[0m\"      # Reset\n    }\n    \n    def __init__(self, name: str = \"leadgen\"):\n        self.name = name\n        self.level = \"INFO\"\n    \n    def _log(self, level: str, message: str, extra: Optional[str] = None):\n        \"\"\"Internal logging method.\"\"\"\n        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n        color = self.COLORS.get(level, \"\")\n        reset = self.COLORS[\"RESET\"]\n        \n        # Format: [HH:MM:SS] LEVEL: message\n        log_msg = f\"[{timestamp}] {color}{level}{reset}: {message}\"\n        \n        if extra:\n            log_msg += f\" - {extra}\"\n            \n        print(log_msg, file=sys.stderr if level == \"ERROR\" else sys.stdout)\n    \n    def debug(self, message: str, extra: Optional[str] = None):\n        \"\"\"Log debug message.\"\"\"\n        if self.level in [\"DEBUG\"]:\n            self._log(\"DEBUG\", message, extra)\n    \n    def info(self, message: str, extra: Optional[str] = None):\n        \"\"\"Log info message.\"\"\"\n        if self.level in [\"DEBUG\", \"INFO\"]:\n            self._log(\"INFO\", message, extra)\n    \n    def warning(self, message: str, extra: Optional[str] = None):\n        \"\"\"Log warning message.\"\"\"\n        if self.level in [\"DEBUG\", \"INFO\", \"WARNING\"]:\n            self._log(\"WARNING\", message, extra)\n    \n    def error(self, message: str, extra: Optional[str] = None):\n        \"\"\"Log error message.\"\"\"\n        self._log(\"ERROR\", message, extra)\n    \n    def success(self, message: str, extra: Optional[str] = None):\n        \"\"\"Log success message in green.\"\"\"\n        self._log(\"INFO\", f\"✓ {message}\", extra)\n\n\n# Global logger instance\nlogger = Logger()","size_bytes":2017},"leadgen/utils/proxy.py":{"content":"import os\nimport requests\nimport time\nfrom typing import Optional, Dict, List, Union\nfrom leadgen.config.loader import ConfigLoader \nfrom leadgen.utils.logging import logger\nfrom leadgen.domain_finders.base import DomainFinderError\n\nclass ProxyError(Exception):\n    \"\"\"Raised when all proxies fail or a proxy request fails.\"\"\"\n    pass\n\nclass ProxyManager:\n    def __init__(self, test_url: str = \"https://api.yelp.com/v3/businesses/search\", test_interval: int = 10, proxies_file=\"config/proxies.txt\"):\n        \"\"\"\n        Args:\n            config: Config loader instance\n            test_url: URL to test proxies against (default Yelp API)\n            test_interval: how many times to use proxy before re-testing\n        \"\"\"\n        self.proxy_index = 0\n        self.use_counts = {}  # track usage count per proxy\n        self.test_url = test_url\n        self.test_interval = test_interval\n        self.proxies: List[str] = ConfigLoader().load_config().proxies\n        self.proxies_file = os.path.join(os.getcwd(), proxies_file)\n\n    def _normalize_proxy(self, proxy_url: str) -> Dict[str, str]:\n        \"\"\"Normalize proxy into requests format.\"\"\"\n        if proxy_url.startswith(\"socks5://\"):\n            proxy_url = proxy_url.replace(\"socks5://\", \"socks5h://\")\n        return {\"http\": proxy_url, \"https\": proxy_url}\n\n    def _test_proxy(self, proxy_url: str) -> bool:\n        \"\"\"Test if a proxy is alive by making a HEAD request.\"\"\"\n        try:\n            response = requests.head(\n                self.test_url,\n                proxies=self._normalize_proxy(proxy_url),\n                timeout=5\n            )\n            return response.status_code < 500\n        except Exception as e:\n            logger.warning(f\"Proxy test failed for {proxy_url}: {e}\")\n            return False\n\n    def _disable_proxy(self, proxy_url: str):\n        \"\"\"Comment out dead proxy in proxies.txt so it won’t be used again.\"\"\"\n        try:\n            with open(self.proxies_file, \"r\") as f:\n                lines = f.readlines()\n            with open(self.proxies_file, \"w\") as f:\n                for line in lines:\n                    if line.strip() == proxy_url and not line.strip().startswith(\"#\"):\n                        f.write(f\"# {line.strip()}  # disabled due to failure\\n\")\n                        logger.info(f\"Disabled proxy {proxy_url}\")\n                    else:\n                        f.write(line)\n        except Exception as e:\n            logger.error(f\"Failed to disable proxy {proxy_url}: {e}\")\n\n    def request(self, method: str, url: str, max_attempts: Optional[int] = None, \n                per_request: bool = True, timeout: int = 10, **kwargs) -> Optional[requests.Response]:\n        \"\"\"\n        Enhanced request method with per-request proxy rotation and retry logic.\n        \n        Args:\n            method: HTTP method (GET, POST, etc.)\n            url: Target URL\n            max_attempts: Maximum retry attempts (default: number of available proxies + 1)\n            per_request: Whether to rotate proxy per request (default: True)\n            timeout: Request timeout in seconds\n            **kwargs: Additional arguments passed to requests\n            \n        Returns:\n            Response object or None if all attempts failed\n            \n        Raises:\n            ProxyError: If all proxies fail\n            requests.HTTPError: For HTTP errors (non-proxy related)\n        \"\"\"\n        if max_attempts is None:\n            max_attempts = len(self.proxies) + 1 if self.proxies else 3\n            \n        last_exception = None\n        \n        for attempt in range(max_attempts):\n            proxy = None\n            if per_request and self.proxies:\n                proxy = self._get_proxy()\n                logger.debug(f\"Attempt {attempt + 1}/{max_attempts}: Using proxy {proxy}\")\n            elif not per_request and 'proxies' not in kwargs and self.proxies:\n                proxy = self._get_proxy()\n                \n            # Set up request parameters\n            request_kwargs = kwargs.copy()\n            if proxy:\n                request_kwargs['proxies'] = proxy\n            request_kwargs['timeout'] = timeout\n            \n            try:\n                response = requests.request(method, url, **request_kwargs)\n                \n                \n                logger.debug(f\"Using proxy {proxy} for {url}\")\n    \n                if response.status_code in [403, 407]:  # Proxy authentication/forbidden\n                    if proxy:\n                        proxy_url = proxy.get('http', proxy.get('https', ''))\n                        logger.warning(f\"Proxy authentication failed (status {response.status_code}): {proxy_url}\")\n                        self._disable_proxy(proxy_url)\n                    continue\n                    \n                response.raise_for_status()\n                logger.debug(f\"Request successful on attempt {attempt + 1}\")\n                return response\n                \n            except requests.exceptions.ProxyError as e:\n                logger.warning(f\"Proxy error on attempt {attempt + 1}: {e}\")\n                if proxy:\n                    proxy_url = proxy.get('http', proxy.get('https', ''))\n                    self._disable_proxy(proxy_url)\n                last_exception = e\n                continue\n                \n            except (requests.exceptions.ConnectTimeout, \n                    requests.exceptions.ReadTimeout,\n                    requests.exceptions.ConnectionError) as e:\n                logger.warning(f\"Connection error on attempt {attempt + 1}: {e}\")\n                if proxy:\n                    proxy_url = proxy.get('http', proxy.get('https', ''))\n                    logger.warning(f\"Disabling proxy due to connection error: {proxy_url}\")\n                    self._disable_proxy(proxy_url)\n                last_exception = e\n                continue\n                \n            except requests.exceptions.HTTPError as e:\n                # Don't disable proxy for HTTP errors - these are server-side issues\n                logger.error(f\"HTTP error on attempt {attempt + 1}: {e}\")\n                raise e\n                \n            except Exception as e:\n                logger.error(f\"Unexpected error on attempt {attempt + 1}: {e}\")\n                last_exception = e\n                continue\n                \n        # All attempts failed\n        if last_exception:\n            if isinstance(last_exception, requests.exceptions.ProxyError):\n                raise ProxyError(f\"All proxy attempts failed. Last error: {last_exception}\")\n            else:\n                raise last_exception\n        else:\n            raise ProxyError(\"All request attempts failed\")\n\n    def _get_proxy(self) -> Optional[Dict[str, str]]:\n        \"\"\"Get next proxy in rotation or None if no proxies.\"\"\"\n        \n        \n        self.proxies = [p for p in self.proxies if p and p.strip()]\n        if not self.proxies:\n            return None\n\n        proxy_url = self.proxies[self.proxy_index % len(self.proxies)].strip()\n        self.proxy_index += 1\n\n        # Track usage\n        self.use_counts[proxy_url] = self.use_counts.get(proxy_url, 0) + 1\n\n        # Retest proxy after threshold\n        if self.use_counts[proxy_url] >= self.test_interval:\n            logger.info(f\"Re-testing proxy {proxy_url}\")\n            if not self._test_proxy(proxy_url):\n                self._disable_proxy(proxy_url)\n                # remove it from rotation\n                self.proxies = [p for p in self.proxies if not p.strip().startswith(\"#\")]\n                return self._get_proxy()  # try next one\n            else:\n                logger.info(f\"Proxy {proxy_url} still working\")\n            self.use_counts[proxy_url] = 0  # reset counter\n\n        return self._normalize_proxy(proxy_url)\n    \n    def safe_request(self, method: str, url: str, **kwargs) -> Optional[requests.Response]:\n        \"\"\"\n        Perform a request with proxy rotation.\n        If a proxy fails, disable it and retry with next proxy.\n        \"\"\"\n        try:\n            response = self.request(method, url, **kwargs)\n            response.raise_for_status()\n            return response\n        except requests.exceptions.ProxyError as e:\n            logger.error(f\"Proxy {kwargs.get('proxies')} failed: {e}, disabling it\")\n            self._disable_proxy(kwargs.get('proxies'))\n            raise ProxyError(f\"Proxy failed: {kwargs.get('proxies')}\") from e\n        except requests.HTTPError as e:\n            logger.error(f\"Request failed: {e}\")\n            raise Exception(f\"Request failed: {e}\") from e","size_bytes":8572},"leadgen/utils/state.py":{"content":"\"\"\"State management for stateful lead generation pipeline.\"\"\"\nimport json\nimport csv\nfrom pathlib import Path\nfrom typing import Set, Dict, Optional, Union\nfrom ..models.company import Company\nfrom ..models.email_result import EmailResult\nfrom ..config.models import OutputConfig\nfrom ..utils.logging import logger\n\n\nclass StateStore:\n    \"\"\"Manages state for resumable lead generation pipeline.\"\"\"\n    \n    def __init__(self, output_dir: Union[str, Path], output_config: OutputConfig):\n        self.output_dir = Path(output_dir)\n        self.config = output_config\n        self.state_dir =  Path(\"./state\")\n        self.state_file = self.state_dir / \"state.json\"\n        \n        # In-memory state\n        self.seen_companies: Set[str] = set()\n        self.seen_domains: Set[str] = set()\n        self.seen_emails: Set[str] = set()\n        \n        # Ensure state directory exists\n        self.state_dir.mkdir(parents=True, exist_ok=True)\n        \n        \n    def load_from_output(self) -> None:\n        \"\"\"Load existing state from output files and cached state.\"\"\"\n        try:\n            # Try to load from cached state first\n            if self.state_file.exists():\n                with open(self.state_file, 'r', encoding='utf-8') as f:\n                    state_data = json.load(f)\n                    self.seen_companies = set(state_data.get('companies', []))\n                    self.seen_domains = set(state_data.get('domains', []))\n                    self.seen_emails = set(state_data.get('emails', []))\n                logger.info(f\"Loaded cached state: {len(self.seen_companies)} companies, {len(self.seen_domains)} domains, {len(self.seen_emails)} emails\")\n                return\n        except Exception as e:\n            logger.warning(f\"Could not load cached state: {e}\")\n            \n        # If cached state not available, load from output files\n        self._load_from_files()\n        \n    def _load_from_files(self) -> None:\n        \"\"\"Load state by reading existing output files.\"\"\"\n        # Load companies\n        self._load_companies()\n        # Load domains  \n        self._load_domains()\n        # Load emails\n        self._load_emails()\n        \n        logger.info(f\"Loaded state from files: {len(self.seen_companies)} companies, {len(self.seen_domains)} domains, {len(self.seen_emails)} emails\")\n        \n    def _load_companies(self) -> None:\n        \"\"\"Load existing companies from output files.\"\"\"\n        formats = [self.config.format.lower()]\n        if self.config.format.lower() in [\"json\", \"jsonl\"]:\n            formats = [\"jsonl\", \"json\"]\n            \n        for fmt in formats:\n            file_path = self.output_dir / f\"{self.config.companies_file}.{fmt}\"\n            if file_path.exists():\n                try:\n                    if fmt == \"csv\":\n                        self._load_companies_csv(file_path)\n                    elif fmt in [\"jsonl\", \"json\"]:\n                        self._load_companies_jsonl(file_path)\n                    elif fmt == \"xlsx\":\n                        self._load_companies_xlsx(file_path)\n                    else:  # txt\n                        self._load_companies_txt(file_path)\n                    break\n                except Exception as e:\n                    logger.warning(f\"Error loading companies from {file_path}: {e}\")\n                    \n    def _normalize_company_key(self, company: Union[Company, Dict]) -> str:\n        \"\"\"Create normalized key for company deduplication.\"\"\"\n        if isinstance(company, Company):\n            name = getattr(company, 'name', '') or \"\"\n            city = getattr(company, 'city', '') or getattr(company, 'location', '') or \"\"\n            address = getattr(company, 'address', '') or \"\"\n            url = getattr(company, 'url', '') or \"\"\n        else:\n            name = company.get('name', '')\n            city = company.get('city', '') or company.get('location', '')\n            address = company.get('address', '')\n            url = company.get('url', '')\n            \n        # Normalize for deduplication (lowercase, strip whitespace)\n        # Use name + location/city as primary key, with address and url as secondary\n        key = f\"{name.lower().strip()}|{city.lower().strip()}|{address.lower().strip()}|{url.lower().strip()}\"\n        return key\n        \n    def _load_companies_csv(self, file_path: Path) -> None:\n        \"\"\"Load companies from CSV file.\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            reader = csv.DictReader(f)\n            for row in reader:\n                key = self._normalize_company_key(row)\n                self.seen_companies.add(key)\n                \n    def _load_companies_jsonl(self, file_path: Path) -> None:\n        \"\"\"Load companies from JSONL file.\"\"\"\n        with open(file_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                if line.strip():\n                    try:\n                        company = json.loads(line.strip())\n                        key = self._normalize_company_key(company)\n                        self.seen_companies.add(key)\n                    except json.JSONDecodeError:\n                        continue\n                        \n    def _load_companies_txt(self, file_path: Path) -> None:\n        \"\"\"Load companies from TXT file (JSON per line).\"\"\"\n        self._load_companies_jsonl(file_path)\n        \n    def _load_companies_xlsx(self, file_path: Path) -> None:\n        \"\"\"Load companies from XLSX file.\"\"\"\n        try:\n            from openpyxl import load_workbook\n            wb = load_workbook(file_path)\n            ws = wb.active\n            if ws is None:\n                return\n                \n            headers = [cell.value for cell in ws[1]]\n            for row in ws.iter_rows(min_row=2, values_only=True):\n                company_dict = dict(zip(headers, row))\n                key = self._normalize_company_key(company_dict)\n                self.seen_companies.add(key)\n        except ImportError:\n            logger.warning(\"openpyxl not available for loading XLSX companies\")\n        except Exception as e:\n            logger.warning(f\"Error loading companies from XLSX: {e}\")\n            \n    def _load_domains(self) -> None:\n        \"\"\"Load existing domains from output files.\"\"\"\n        # Try TXT first, then CSV\n        for ext in ['txt', 'csv']:\n            file_path = self.output_dir / f\"{self.config.domains_file}.{ext}\"\n            if file_path.exists():\n                try:\n                    if ext == 'csv':\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            reader = csv.reader(f)\n                            next(reader, None)  # Skip header\n                            for row in reader:\n                                if row and row[0].strip():\n                                    self.seen_domains.add(row[0].strip())\n                    else:  # txt\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            for line in f:\n                                domain = line.strip()\n                                if domain:\n                                    self.seen_domains.add(domain)\n                    break\n                except Exception as e:\n                    logger.warning(f\"Error loading domains from {file_path}: {e}\")\n                    \n    def _load_emails(self) -> None:\n        \"\"\"Load existing emails from output files.\"\"\"\n        formats = [self.config.format.lower()]\n        if self.config.format.lower() in [\"json\", \"jsonl\"]:\n            formats = [\"jsonl\", \"json\"]\n            \n        for fmt in formats:\n            file_path = self.output_dir / f\"{self.config.emails_file}.{fmt}\"\n            if file_path.exists():\n                try:\n                    if fmt == \"csv\":\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            reader = csv.DictReader(f)\n                            for row in reader:\n                                email = row.get('email', '').strip()\n                                if email:\n                                    self.seen_emails.add(email.lower())\n                    elif fmt in [\"jsonl\", \"json\"]:\n                        with open(file_path, 'r', encoding='utf-8') as f:\n                            for line in f:\n                                if line.strip():\n                                    try:\n                                        email_data = json.loads(line.strip())\n                                        email = email_data.get('email', '').strip()\n                                        if email:\n                                            self.seen_emails.add(email.lower())\n                                    except json.JSONDecodeError:\n                                        continue\n                    elif fmt == \"xlsx\":\n                        try:\n                            from openpyxl import load_workbook\n                            wb = load_workbook(file_path)\n                            ws = wb.active\n                            if ws is None:\n                                continue\n                                \n                            headers = [cell.value for cell in ws[1]]\n                            email_col = None\n                            for i, header in enumerate(headers):\n                                if header and 'email' in header.lower():\n                                    email_col = i\n                                    break\n                                    \n                            if email_col is not None:\n                                for row in ws.iter_rows(min_row=2, values_only=True):\n                                    if len(row) > email_col and row[email_col]:\n                                        email = str(row[email_col]).strip()\n                                        if email:\n                                            self.seen_emails.add(email.lower())\n                        except ImportError:\n                            logger.warning(\"openpyxl not available for loading XLSX emails\")\n                    break\n                except Exception as e:\n                    logger.warning(f\"Error loading emails from {file_path}: {e}\")\n                    \n    def is_seen_company(self, company: Company) -> bool:\n        \"\"\"Check if company has been processed before.\"\"\"\n        key = self._normalize_company_key(company)\n        return key in self.seen_companies\n        \n    def is_seen_domain(self, domain: str) -> bool:\n        \"\"\"Check if domain has been processed before.\"\"\"\n        return domain.strip() in self.seen_domains\n        \n    def is_seen_email(self, email: str) -> bool:\n        \"\"\"Check if email has been found before.\"\"\"\n        return email.lower().strip() in self.seen_emails\n        \n    def add_seen_company(self, company: Company) -> None:\n        \"\"\"Mark company as seen.\"\"\"\n        key = self._normalize_company_key(company)\n        self.seen_companies.add(key)\n        \n    def add_seen_domain(self, domain: str) -> None:\n        \"\"\"Mark domain as seen.\"\"\"\n        self.seen_domains.add(domain.strip())\n        \n    def add_seen_email(self, email: str) -> None:\n        \"\"\"Mark email as seen.\"\"\"\n        self.seen_emails.add(email.lower().strip())\n        \n    def save_state(self) -> None:\n        \"\"\"Persist state to cache file.\"\"\"\n        try:\n            state_data = {\n                'companies': list(self.seen_companies),\n                'domains': list(self.seen_domains),\n                'emails': list(self.seen_emails)\n            }\n            with open(self.state_file, 'w', encoding='utf-8') as f:\n                json.dump(state_data, f, ensure_ascii=False, indent=2)\n            logger.debug(f\"Saved state to {self.state_file}\")\n        except Exception as e:\n            logger.warning(f\"Could not save state: {e}\")\n            \n    def get_stats(self) -> Dict[str, int]:\n        \"\"\"Get statistics about current state.\"\"\"\n        return {\n            'companies': len(self.seen_companies),\n            'domains': len(self.seen_domains),\n            'emails': len(self.seen_emails)\n        }\n        \n    def clear_state(self) -> None:\n        \"\"\"Clear all state (for fresh runs).\"\"\"\n        self.seen_companies.clear()\n        self.seen_domains.clear()\n        self.seen_emails.clear()\n        if self.state_file.exists():\n            self.state_file.unlink()\n        logger.info(\"Cleared all state for fresh run\")","size_bytes":12481},"replit.md":{"content":"# Overview\n\nThis is a Python-based lead generation application that automates the process of finding businesses and their contact information. The system searches for companies using various providers (currently Yelp, with Google support planned), extracts business domains, and then discovers email addresses for those domains using email discovery services like Hunter.io. The application follows a modular pipeline architecture with separate components for search providers, domain resolution, and email discovery.\n\n# User Preferences\n\nPreferred communication style: Simple, everyday language.\n\n# System Architecture\n\n## Core Components\n\nThe application follows a modular pipeline architecture with clearly separated concerns:\n\n**CLI Interface**: Simple command-line interface (`leadgen/cli/main.py`) that orchestrates the entire lead generation process with configurable parameters like delay times, output directories, and location filtering.\n\n**Orchestrator Pattern**: The `LeadOrchestrator` class manages the complete workflow, coordinating between different providers and finders while maintaining state and handling errors gracefully.\n\n**Provider Architecture**: Pluggable search providers implement the `BaseProvider` interface to find companies. Currently supports Yelp with extensible design for additional providers like Google.\n\n**Email Discovery System**: Modular email finders implement the `BaseFinder` interface. Currently supports Hunter.io with plans for Snov integration.\n\n**Domain Resolution**: The `DomainResolver` utility extracts business domains from provider URLs, filtering out social media and platform domains to focus on actual business websites.\n\n**Configuration Management**: File-based configuration system that loads API keys, search queries, and settings from text files in the `config/` directory, with environment variable overrides.\n\n**State Management**: Persistent state tracking to enable resumable operations and avoid duplicate processing of companies, domains, and emails.\n\n**Output Management**: Flexible output system supporting multiple formats (JSON, CSV, Excel) with structured data organization.\n\n## Data Flow\n\n1. **Configuration Loading**: API keys and settings loaded from config files\n2. **Company Search**: Providers search for businesses based on configured queries\n3. **Domain Extraction**: Business domains extracted from company URLs, excluding platform domains\n4. **Email Discovery**: Email finders discover contact information for extracted domains\n5. **Result Storage**: All data saved to output files with state tracking for resumability\n\n## Error Handling & Resilience\n\nThe system implements comprehensive error handling with proxy rotation support, rate limiting awareness, and graceful degradation when services are unavailable. The state management system allows for interrupted processes to resume without losing progress.\n\n# External Dependencies\n\n## Search Providers\n\n**Yelp Fusion API**: Primary business search provider requiring API key authentication. Used to discover businesses based on location and search terms.\n\n**Google Custom Search API**: Planned integration for additional business discovery (currently commented out in codebase).\n\n## Email Discovery Services\n\n**Hunter.io API**: Primary email discovery service that finds email addresses associated with company domains. Requires API key and supports department filtering.\n\n**Snov.io API**: Planned secondary email discovery service (infrastructure in place but not fully implemented).\n\n## Data Storage\n\n**File-based Storage**: JSON, CSV, and Excel output formats for results storage. No external database dependencies.\n\n**State Persistence**: JSON-based state files for tracking processed companies, domains, and emails to enable resumable operations.\n\n## HTTP Infrastructure\n\n**Requests Library**: HTTP client for all API communications with built-in proxy support and error handling.\n\n**Proxy Management**: Optional SOCKS5 proxy support for request routing (currently disabled due to missing dependencies).\n\n## Development Tools\n\n**OpenPyXL**: Excel file generation for structured output formatting.\n\n**Standard Library**: Heavy reliance on Python standard library components (pathlib, json, csv, argparse) for core functionality.","size_bytes":4267}},"version":1}